# Locally
1. poetry shell
2. streamlit run app/main.py

# Docker
1. docker build -t streamlit-poetry-app .
2. docker run -p 8501:8501 streamlit-poetry-app

# LLM MODEL (Local)
Ollama (llama3)

## TODO: 
1. put Ollama in Docker
2. choose lighter model (mistral) and for embeddings 

